{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b32d0897-7e7b-4d0f-b645-8c45bd24b745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding lookalike customers\n",
      "\n",
      "Results saved to FAISS_FirstName_LastName_Lookalike.csv\n",
      "\n",
      "Lookalike Results for Customers C0001-C0020:\n",
      "   cust_id                           lookalikes\n",
      "0    C0001  C0069:0.911,C0183:0.899,C0023:0.895\n",
      "1    C0002  C0159:0.904,C0178:0.903,C0134:0.900\n",
      "2    C0003  C0007:0.900,C0166:0.885,C0005:0.884\n",
      "3    C0004  C0075:0.921,C0122:0.920,C0028:0.917\n",
      "4    C0005  C0031:0.899,C0166:0.897,C0085:0.886\n",
      "5    C0006  C0135:0.911,C0079:0.909,C0196:0.905\n",
      "6    C0007  C0003:0.900,C0166:0.893,C0085:0.891\n",
      "7    C0008  C0098:0.916,C0162:0.914,C0175:0.913\n",
      "8    C0009  C0032:0.858,C0198:0.833,C0088:0.829\n",
      "9    C0010  C0111:0.909,C0132:0.908,C0030:0.905\n",
      "10   C0011  C0056:0.916,C0188:0.914,C0099:0.913\n",
      "11   C0012  C0113:0.916,C0162:0.914,C0195:0.914\n",
      "12   C0013  C0046:0.917,C0053:0.917,C0016:0.915\n",
      "13   C0014  C0060:0.889,C0080:0.886,C0110:0.878\n",
      "14   C0015  C0071:0.889,C0052:0.853,C0025:0.843\n",
      "15   C0016  C0013:0.915,C0064:0.912,C0053:0.911\n",
      "16   C0017  C0156:0.912,C0101:0.912,C0113:0.912\n",
      "17   C0018  C0125:0.904,C0114:0.904,C0045:0.900\n",
      "18   C0019  C0191:0.905,C0017:0.899,C0179:0.896\n",
      "19   C0020  C0120:0.815,C0066:0.807,C0130:0.784\n"
     ]
    }
   ],
   "source": [
    "#to find lookalikes, i thought of two approaches. one is overkill and\n",
    "#other is normal.\n",
    "#the overkill one first: Facebook AI Similarity Search\n",
    "#uses an Index datastructure to find similar ones, need to store vector embeddings #\n",
    "# / feature vectors in our case \n",
    "#for that purpose.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime\n",
    "\n",
    "customers_df=pd.read_csv('Customers Data.csv')\n",
    "transactions_df=pd.read_csv('Transactions.csv')\n",
    "products_df=pd.read_csv('Products.csv')\n",
    "def create_balanced_features(customers_df, transactions_df, products_df):\n",
    "    #create balanced feature vectors for customer similarity\n",
    "    \n",
    "    # convert dates\n",
    "    customers_df['SignupDate'] = pd.to_datetime(customers_df['SignupDate'])\n",
    "    transactions_df['TransactionDate'] = pd.to_datetime(transactions_df['TransactionDate'])\n",
    "    current_date = transactions_df['TransactionDate'].max()\n",
    "    \n",
    "    # 1. Recency Frequency Monetary Score (40% weight)\n",
    "    rfm = transactions_df.groupby('CustomerID').agg({\n",
    "        'TransactionDate': lambda x: (current_date - x.max()).days,  \n",
    "        'TransactionID': 'count',                                    \n",
    "        'TotalValue': 'sum'                                         \n",
    "    })\n",
    "    rfm.columns = ['Recency', 'Frequency', 'Monetary']\n",
    "    \n",
    "    # Convert to scores by equally dividing into quintiles and explicitly convert to numeric\n",
    "    rfm['RecencyScore'] = pd.qcut(rfm['Recency'].rank(method='first'), \n",
    "                                 q=5, labels=[5, 4, 3, 2, 1]).astype(float)\n",
    "    # here what we did is that we used qcut to equally divide into quintiles of 5 groups, 5=very recent=better\n",
    "    rfm['FrequencyScore'] = pd.qcut(rfm['Frequency'].rank(method='first'), \n",
    "                                   q=5, labels=[1, 2, 3, 4, 5]).astype(float)\n",
    "    rfm['MonetaryScore'] = pd.qcut(rfm['Monetary'].rank(method='first'), \n",
    "                                  q=5, labels=[1, 2, 3, 4, 5]).astype(float)\n",
    "    \n",
    "    # Order Statistics (20% weight)\n",
    "    order_stats = transactions_df.groupby('CustomerID').agg({\n",
    "        'TotalValue': ['mean', 'std'],\n",
    "        'Quantity': ['mean', 'std']\n",
    "    }).fillna(0)\n",
    "    order_stats.columns = ['AvgOrderValue', 'StdOrderValue', \n",
    "                          'AvgQuantity', 'StdQuantity']\n",
    "    \n",
    "    # Category Preferences (30% weight)\n",
    "    enriched_transactions = transactions_df.merge(products_df, on='ProductID')\n",
    "    category_pivot = pd.crosstab(\n",
    "        enriched_transactions['CustomerID'], \n",
    "        enriched_transactions['Category'],\n",
    "        values=enriched_transactions['TotalValue'],\n",
    "        aggfunc='sum',\n",
    "        normalize='index'\n",
    "    ).fillna(0)\n",
    "    \n",
    "    # Purchase Timing (10% weight)\n",
    "    timing_features = transactions_df.groupby('CustomerID').agg({\n",
    "        'TransactionDate': [\n",
    "            lambda x: x.dt.hour.mean(),     # Average purchase hour\n",
    "            lambda x: x.dt.dayofweek.mean(), # Average day of week\n",
    "            lambda x: x.dt.dayofweek.std(),  # Variation in purchase day\n",
    "            lambda x: x.diff().dt.days.mean() # Average days between purchases\n",
    "        ]\n",
    "    }).fillna(0)\n",
    "    timing_features.columns = ['AvgHour', 'AvgDayOfWeek', \n",
    "                             'DayVariation', 'PurchaseInterval']\n",
    "    \n",
    "    # Multiply features by their weights\n",
    "    rfm_weighted = rfm[['RecencyScore', 'FrequencyScore', 'MonetaryScore']].multiply(0.4)\n",
    "    order_stats_weighted = order_stats.multiply(0.2)\n",
    "    category_pivot_weighted = category_pivot.multiply(0.3)\n",
    "    timing_features_weighted = timing_features.multiply(0.1)\n",
    "    \n",
    "    # Combine all features\n",
    "    customer_features = pd.concat([\n",
    "        rfm_weighted,\n",
    "        order_stats_weighted,\n",
    "        category_pivot_weighted,\n",
    "        timing_features_weighted\n",
    "    ], axis=1).fillna(0)\n",
    "    \n",
    "    return customer_features\n",
    "\n",
    "def find_balanced_lookalikes(customer_features, target_customers, k=3):\n",
    "    # Find lookalike customers using FAISS\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_features = scaler.fit_transform(customer_features.values.astype('float32'))\n",
    "    normalized_features = np.ascontiguousarray(normalized_features)\n",
    "    \n",
    "    # Build FAISS index\n",
    "    dimension = normalized_features.shape[1]\n",
    "    faiss.normalize_L2(normalized_features)\n",
    "    index = faiss.IndexFlatIP(dimension)\n",
    "    index.add(normalized_features)\n",
    "    \n",
    "    customer_ids = customer_features.index.tolist()\n",
    "    results = {}\n",
    "    \n",
    "    for target_id in target_customers:\n",
    "            target_idx = customer_ids.index(target_id)\n",
    "            query_vector = np.ascontiguousarray(normalized_features[target_idx:target_idx+1])\n",
    "            \n",
    "            # find similar ones\n",
    "            D, I = index.search(query_vector, k + 1)\n",
    "            \n",
    "            # results\n",
    "            recommendations = []\n",
    "            for sim, idx in zip(D[0], I[0]):\n",
    "                if customer_ids[idx] != target_id:\n",
    "                    # Transform similarity score\n",
    "                    similarity_score = 1 / (1 + np.exp(-5 * (sim - 0.5)))\n",
    "                    recommendations.append(f\"{customer_ids[idx]}:{similarity_score:.3f}\")\n",
    "                if len(recommendations) == k:\n",
    "                    break\n",
    "                    \n",
    "            results[target_id] = recommendations\n",
    "            \n",
    "        \n",
    "            \n",
    "    return results\n",
    "\n",
    "customer_features = create_balanced_features(customers_df, transactions_df, products_df)\n",
    "#  target customers 1 to 20\n",
    "target_customers = [f'C{str(i).zfill(4)}' for i in range(1, 21)]\n",
    "# Find lookalikes\n",
    "print(\"finding lookalike customers\")\n",
    "results = find_balanced_lookalikes(customer_features, target_customers)\n",
    "# Create output DataFrame\n",
    "final_df = pd.DataFrame({\n",
    "        'cust_id': list(results.keys()),\n",
    "        'lookalikes': [','.join(v) for v in results.values()]\n",
    "    })\n",
    "    \n",
    "# save to CSV\n",
    "output_filename = 'FAISS_FirstName_LastName_Lookalike.csv'\n",
    "final_df.to_csv(output_filename, index=False)\n",
    "print(f\"\\nResults saved to {output_filename}\")\n",
    "    \n",
    "\n",
    "print(\"\\nLookalike Results for Customers C0001-C0020:\")\n",
    "print(final_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "266c11f1-deb3-40d9-82df-3933e02d7b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results saved to Approach_Classic_FirstName_LastName_Lookalike.csv\n",
      "\n",
      "Lookalike Results for Customers C0001-C0020:\n",
      "   cust_id                                                   lookalikes\n",
      "0    C0001  C0005:0.744,C0069:0.676,C0020:0.656,C0091:0.560,C0181:0.506\n",
      "1    C0002  C0106:0.641,C0060:0.629,C0151:0.617,C0077:0.585,C0086:0.570\n",
      "2    C0003  C0144:0.752,C0007:0.646,C0091:0.608,C0005:0.586,C0045:0.544\n",
      "3    C0004  C0075:0.887,C0122:0.813,C0028:0.770,C0065:0.727,C0162:0.700\n",
      "4    C0005  C0001:0.744,C0128:0.646,C0094:0.633,C0031:0.627,C0199:0.618\n",
      "5    C0006  C0135:0.708,C0079:0.700,C0170:0.613,C0149:0.586,C0129:0.574\n",
      "6    C0007  C0166:0.676,C0003:0.646,C0112:0.622,C0120:0.609,C0005:0.585\n",
      "7    C0008  C0098:0.851,C0162:0.726,C0175:0.679,C0145:0.656,C0194:0.648\n",
      "8    C0009  C0058:0.762,C0032:0.751,C0150:0.709,C0097:0.694,C0119:0.675\n",
      "9    C0010  C0132:0.774,C0027:0.771,C0111:0.764,C0198:0.734,C0030:0.704\n",
      "10   C0011  C0099:0.815,C0188:0.754,C0101:0.710,C0165:0.620,C0056:0.613\n",
      "11   C0012  C0113:0.804,C0104:0.736,C0163:0.722,C0162:0.706,C0152:0.664\n",
      "12   C0013  C0188:0.735,C0099:0.723,C0053:0.691,C0169:0.678,C0093:0.651\n",
      "13   C0014  C0060:0.837,C0080:0.832,C0110:0.773,C0097:0.704,C0151:0.700\n",
      "14   C0015  C0071:0.613,C0097:0.585,C0009:0.554,C0127:0.554,C0197:0.553\n",
      "15   C0016  C0021:0.663,C0126:0.632,C0051:0.622,C0187:0.606,C0191:0.582\n",
      "16   C0017  C0156:0.657,C0041:0.621,C0090:0.570,C0101:0.559,C0068:0.516\n",
      "17   C0018  C0114:0.686,C0148:0.609,C0087:0.604,C0045:0.562,C0176:0.540\n",
      "18   C0019  C0147:0.740,C0139:0.711,C0047:0.697,C0119:0.605,C0121:0.593\n",
      "19   C0020  C0001:0.656,C0066:0.637,C0026:0.600,C0055:0.571,C0120:0.527\n"
     ]
    }
   ],
   "source": [
    "# now the classical approach\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import cdist\n",
    " # Standardize features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(customer_features)\n",
    "target_customers = [f'C{str(i).zfill(4)}' for i in range(1, 21)]\n",
    "similarity_matrix = 1 - cdist(scaled_features, scaled_features, metric='cosine')\n",
    "# Find similar customers using cosine similarity\"\"\"\n",
    "# Create DataFrame for easier lookup\n",
    "similarity_df = pd.DataFrame(\n",
    "        similarity_matrix,\n",
    "        index=customer_features.index,\n",
    "        columns=customer_features.index\n",
    "    )    \n",
    "results = {}\n",
    "for target_id in target_customers:\n",
    "        # Get similarities for target customer\n",
    "        customer_similarities = similarity_df[target_id].sort_values(ascending=False)\n",
    "        similar_customers = customer_similarities[customer_similarities.index != target_id][:5] # no.of recommendations is now set to 5\n",
    "        recommendations = [f\"{idx}:{score:.3f}\" for idx, score in similar_customers.items()]\n",
    "        results[target_id] = recommendations\n",
    "# output DataFrame\n",
    "final_df = pd.DataFrame({\n",
    "        'cust_id': list(results.keys()),\n",
    "        'lookalikes': [','.join(v) for v in results.values()]\n",
    "    })\n",
    "    \n",
    "# save results\n",
    "output_filename = 'Approach_Classic_FirstName_LastName_Lookalike.csv'\n",
    "final_df.to_csv(output_filename, index=False)\n",
    "print(f\"\\nResults saved to {output_filename}\")\n",
    "# Display results\n",
    "print(\"\\nLookalike Results for Customers C0001-C0020:\")\n",
    "print(final_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa100ea-57e7-49d4-9044-aaf9524f28d7",
   "metadata": {},
   "source": [
    "### cdist calculates pairwise distances between all customers\n",
    "### metric='cosine' uses cosine distance\n",
    "### 1 - cdist converts distance to similarity (higher means more similar)\n",
    "### For each target customer:\n",
    "\n",
    "Gets their similarity scores with all other customers\n",
    "Sorts by similarity (highest first)\n",
    "Removes the customer themselves\n",
    "Takes top 5 most similar\n",
    "Formats as \"CustomerID:SimilarityScore\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea634435-a44b-4643-b649-c1b59adf9261",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
